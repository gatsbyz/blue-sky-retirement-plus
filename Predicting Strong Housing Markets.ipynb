{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model to Predict Strong Housing Markets\n",
    "\n",
    "This model will assess past characteristics of strong markets to predict future performance in 400+ US metropolitan areas.  \n",
    "\n",
    "## About the model:\n",
    "\n",
    "\n",
    "1. Logistic Regression Supervised Learning Model used to identify the top 90th percentile markets based on home price index growth.  \n",
    "\n",
    "2. Employment data from the BLS was used to compute y/y growth for each of the 400+ markets. \n",
    "\n",
    "3. Home price index (HPI) data was drawn from the FHFA to compute y/y growth in home prices in each of the 400_ markets.  \n",
    "\n",
    "4. Data sets were indexed by the MSA names which were manually adjusted to easily merge the two data sets for analysis.  \n",
    "\n",
    "5. Lag variables were added as features for HPI and employment.  \n",
    "\n",
    "6. Employment and HPI data were grouped into periods of five years starting from 1991 to 2021.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required modules\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the `project_2_data_2.csv` file from the `Resources` folder into a Pandas DataFrame. Set the “MSA” column as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>%hpi</th>\n",
       "      <th>rank</th>\n",
       "      <th>%hpi_lag</th>\n",
       "      <th>Employment</th>\n",
       "      <th>Employment_ Lag</th>\n",
       "      <th>Period</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSA</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Abilene, TX</th>\n",
       "      <td>0.149704</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.009124</td>\n",
       "      <td>0.68</td>\n",
       "      <td>2.26</td>\n",
       "      <td>1996-2000</td>\n",
       "      <td>0.028429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Albany, GA</th>\n",
       "      <td>0.126767</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.007567</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.76</td>\n",
       "      <td>1996-2000</td>\n",
       "      <td>0.036046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Albany-Schenectady-Troy, NY</th>\n",
       "      <td>0.107624</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.014653</td>\n",
       "      <td>1.66</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>1996-2000</td>\n",
       "      <td>0.008185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Albuquerque, NM</th>\n",
       "      <td>0.106139</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.006384</td>\n",
       "      <td>2.22</td>\n",
       "      <td>3.84</td>\n",
       "      <td>1996-2000</td>\n",
       "      <td>0.099915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alexandria, LA</th>\n",
       "      <td>0.100375</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>1.64</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1996-2000</td>\n",
       "      <td>-0.020036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 %hpi  rank  %hpi_lag  Employment  \\\n",
       "MSA                                                                 \n",
       "Abilene, TX                  0.149704     1 -0.009124        0.68   \n",
       "Albany, GA                   0.126767     1 -0.007567        0.04   \n",
       "Albany-Schenectady-Troy, NY  0.107624     1 -0.014653        1.66   \n",
       "Albuquerque, NM              0.106139     1 -0.006384        2.22   \n",
       "Alexandria, LA               0.100375     1  0.000399        1.64   \n",
       "\n",
       "                             Employment_ Lag    Period   Population   \n",
       "MSA                                                                   \n",
       "Abilene, TX                             2.26  1996-2000     0.028429  \n",
       "Albany, GA                              2.76  1996-2000     0.036046  \n",
       "Albany-Schenectady-Troy, NY            -0.06  1996-2000     0.008185  \n",
       "Albuquerque, NM                         3.84  1996-2000     0.099915  \n",
       "Alexandria, LA                          2.00  1996-2000    -0.020036  "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the project_2_data.csv file into a PandasDataFrame.\n",
    "housing_data = pd.read_csv(\n",
    "    Path('./Resources/project_2_data_2.csv'), \n",
    "    index_col='MSA'\n",
    ")\n",
    "\n",
    "# Review the DataFrame\n",
    "housing_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2: Assess the structure of the target to determine balance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1221\n",
       "1     125\n",
       "Name: rank, dtype: int64"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the value_counts() function to determine the distribution of the \"0\" and \"1\" values. \n",
    "\n",
    "housing_data['rank'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Using the `project_2_data_2.csv` DataFrame, separate the data into training and testing data. Start by defining the `target` (the “rank” column) and the `features` of the data (all the columns except “rank”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The target column should be the binary `rank` column.\n",
    "y = housing_data['rank']\n",
    "\n",
    "\n",
    "# The features column should be all of the features. \n",
    "X = housing_data.drop(columns=['rank','Period ','%hpi']).dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Split the features and target data into `training_features`, `testing_features`, `training_targets`, and `testing_targets` datasets by using the `train_test_split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset using the train_test_split function\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Use scikit-learn's `StandardScaler` to scale the features data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the features training dataset\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Fit the scaler to the features training dataset\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Oversample the data to reduce imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the data\n",
    "\n",
    "random_oversampler = RandomOverSampler(random_state=1)\n",
    "X_resampled, y_resampled = random_oversampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    913\n",
       "0    913\n",
       "Name: rank, dtype: int64"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the distinct values\n",
    "y_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Fit the Data to a Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Declare a `LogisticRegression` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a logistic regression model.\n",
    "# Apply a random_state of 7 to the model\n",
    "logistic_regression_model = LogisticRegression(random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Fit the training data to the model, and save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and save the logistic regression model using the training data\n",
    "lr_model = logistic_regression_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the Testing Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Make predictions about fraud by using the testing dataset, and save those predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make and save testing predictions with the saved logistic regression model using the test data\n",
    "testing_predections = lr_model.predict(X_test)\n",
    "\n",
    "# Review the predictions\n",
    "testing_predections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Calculate the accuracy score by evaluating `testing_targets` vs. `testing_predictions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9109792284866469"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the accuracy score for the test dataset.\n",
    "accuracy_score(y_test, testing_predections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the strongest housing market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>%hpi</th>\n",
       "      <th>rank</th>\n",
       "      <th>%hpi_lag</th>\n",
       "      <th>Employment</th>\n",
       "      <th>Employment_ Lag</th>\n",
       "      <th>Period</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSA</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pine Bluff, AR</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>2016-2021</td>\n",
       "      <td>-0.013959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Danville, IL</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-1.47</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>2016-2021</td>\n",
       "      <td>-0.013204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chico, CA</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>2.18</td>\n",
       "      <td>2016-2021</td>\n",
       "      <td>-0.012199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beckley, WV</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>-1.12</td>\n",
       "      <td>2016-2021</td>\n",
       "      <td>-0.011769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Goldsboro, NC</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2016-2021</td>\n",
       "      <td>-0.010171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                %hpi  rank  %hpi_lag  Employment  Employment_ Lag    Period   \\\n",
       "MSA                                                                            \n",
       "Pine Bluff, AR  0.04     0      0.00       -0.97            -2.02  2016-2021   \n",
       "Danville, IL    0.04     0      0.01       -1.47            -0.08  2016-2021   \n",
       "Chico, CA       0.09     0      0.03       -0.43             2.18  2016-2021   \n",
       "Beckley, WV     0.04     0      0.01       -0.60            -1.12  2016-2021   \n",
       "Goldsboro, NC   0.06     0     -0.01       -0.38             0.12  2016-2021   \n",
       "\n",
       "                Population  \n",
       "MSA                         \n",
       "Pine Bluff, AR   -0.013959  \n",
       "Danville, IL     -0.013204  \n",
       "Chico, CA        -0.012199  \n",
       "Beckley, WV      -0.011769  \n",
       "Goldsboro, NC    -0.010171  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the predicting_data.csv file into a PandasDataFrame.\n",
    "predicting_data = pd.read_csv(\n",
    "    Path(\"./Resources/predicting_data.csv\"), \n",
    "    index_col=\"MSA\"\n",
    ")\n",
    "\n",
    "# Review the DataFrame\n",
    "predicting_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = predicting_data.drop(columns=['rank','Period ','%hpi']).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>%hpi_lag</th>\n",
       "      <th>Employment</th>\n",
       "      <th>Employment_ Lag</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSA</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pine Bluff, AR</th>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>-0.013959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Danville, IL</th>\n",
       "      <td>0.01</td>\n",
       "      <td>-1.47</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.013204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chico, CA</th>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>2.18</td>\n",
       "      <td>-0.012199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beckley, WV</th>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>-1.12</td>\n",
       "      <td>-0.011769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Goldsboro, NC</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-0.010171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Myrtle Beach-Conway-North Myrtle Beach, SC-NC</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>2.17</td>\n",
       "      <td>2.16</td>\n",
       "      <td>0.028453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Greeley, CO</th>\n",
       "      <td>0.07</td>\n",
       "      <td>1.38</td>\n",
       "      <td>4.54</td>\n",
       "      <td>0.029383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Provo-Orem, UT</th>\n",
       "      <td>0.04</td>\n",
       "      <td>4.30</td>\n",
       "      <td>5.28</td>\n",
       "      <td>0.030201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Coeur d'Alene, ID</th>\n",
       "      <td>0.03</td>\n",
       "      <td>3.02</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.031405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>St. George, UT</th>\n",
       "      <td>0.04</td>\n",
       "      <td>4.85</td>\n",
       "      <td>5.22</td>\n",
       "      <td>0.036049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>394 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               %hpi_lag  Employment  \\\n",
       "MSA                                                                   \n",
       "Pine Bluff, AR                                     0.00       -0.97   \n",
       "Danville, IL                                       0.01       -1.47   \n",
       "Chico, CA                                          0.03       -0.43   \n",
       "Beckley, WV                                        0.01       -0.60   \n",
       "Goldsboro, NC                                     -0.01       -0.38   \n",
       "...                                                 ...         ...   \n",
       "Myrtle Beach-Conway-North Myrtle Beach, SC-NC     -0.01        2.17   \n",
       "Greeley, CO                                        0.07        1.38   \n",
       "Provo-Orem, UT                                     0.04        4.30   \n",
       "Coeur d'Alene, ID                                  0.03        3.02   \n",
       "St. George, UT                                     0.04        4.85   \n",
       "\n",
       "                                               Employment_ Lag  Population  \n",
       "MSA                                                                         \n",
       "Pine Bluff, AR                                           -2.02   -0.013959  \n",
       "Danville, IL                                             -0.08   -0.013204  \n",
       "Chico, CA                                                 2.18   -0.012199  \n",
       "Beckley, WV                                              -1.12   -0.011769  \n",
       "Goldsboro, NC                                             0.12   -0.010171  \n",
       "...                                                        ...         ...  \n",
       "Myrtle Beach-Conway-North Myrtle Beach, SC-NC             2.16    0.028453  \n",
       "Greeley, CO                                               4.54    0.029383  \n",
       "Provo-Orem, UT                                            5.28    0.030201  \n",
       "Coeur d'Alene, ID                                         2.50    0.031405  \n",
       "St. George, UT                                            5.22    0.036049  \n",
       "\n",
       "[394 rows x 4 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wonkyung\\anaconda3\\envs\\dev\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names unseen at fit time:\n",
      "- Population\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- Population \n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make and save predictions with the saved logistic regression model\n",
    "\n",
    "mythreshold = 0.15\n",
    "#y_pred = lr_model.predict(X_new)  \n",
    "y_pred =  (lr_model.predict_proba(X_new)[:,1]>=mythreshold).astype(int)\n",
    "\n",
    "# Review the predictions\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSA</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pine Bluff, AR</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Danville, IL</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chico, CA</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beckley, WV</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Goldsboro, NC</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Myrtle Beach-Conway-North Myrtle Beach, SC-NC</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Greeley, CO</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Provo-Orem, UT</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Coeur d'Alene, ID</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>St. George, UT</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>394 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               rank\n",
       "MSA                                                \n",
       "Pine Bluff, AR                                    0\n",
       "Danville, IL                                      0\n",
       "Chico, CA                                         0\n",
       "Beckley, WV                                       0\n",
       "Goldsboro, NC                                     0\n",
       "...                                             ...\n",
       "Myrtle Beach-Conway-North Myrtle Beach, SC-NC     0\n",
       "Greeley, CO                                       0\n",
       "Provo-Orem, UT                                    0\n",
       "Coeur d'Alene, ID                                 0\n",
       "St. George, UT                                    1\n",
       "\n",
       "[394 rows x 1 columns]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred = pd.DataFrame(y_pred,\n",
    "                       index = X_new.index,\n",
    "                       columns = ['rank']\n",
    "                      )\n",
    "\n",
    "df_pred\n",
    "#df_pred.reindex\n",
    "#df_pred.set_index(X_new.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rank\n",
       "0       386\n",
       "1         8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count the number of strong markets predicted. \n",
    "df_pred.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output dataframe to csv file. \n",
    "\n",
    "df_pred.to_csv('strong_markets_2025')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
